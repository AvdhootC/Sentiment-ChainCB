
import os
import google.generativeai as genai
from flask import Flask, request, jsonify, render_template, session, url_for
from dotenv import load_dotenv
import logging
import json # Import json for handling potential complex data structures

# Configure logging
logging.basicConfig(level=logging.INFO)

# Load environment variables from .env file
load_dotenv()

app = Flask(__name__)
# --- IMPORTANT: Set a strong secret key from environment variable ---
app.secret_key = os.environ.get("FLASK_SECRET_KEY")
if not app.secret_key:
    logging.warning("FLASK_SECRET_KEY not set in .env, using a default (INSECURE for production)")
    app.secret_key = "default-insecure-secret-key" # Avoid this in production

# --- Configure Gemini API ---
api_key = os.getenv("GEMINI_API_KEY")
genai_configured = False
if not api_key:
    logging.error("GEMINI_API_KEY not found in environment variables. Chatbot will be disabled.")
else:
    try:
        genai.configure(api_key=api_key)
        genai_configured = True
        logging.info("Gemini API configured successfully.")
    except Exception as e:
        logging.error(f"Error configuring Gemini API: {e}. Chatbot will be disabled.")

# --- No need for data fetching functions here, context comes from frontend ---

# --- Flask Routes ---

@app.route('/')
def index():
    """ Serves the main HTML page. """
    # --- !!! Placeholder Authentication !!! ---
    # In a real app, integrate your actual user login system here.
    # This simulates a user being logged in for the demo.
    if 'user_id' not in session:
        session['user_id'] = f"user_{os.urandom(4).hex()}" # Simple session-based user ID
        logging.info(f"New session started for user_id: {session['user_id']}")
    # ------------------------------------------
    return render_template('index.html')

@app.route('/api/chat', methods=['POST'])
def handle_chat():
    """ Handles chat messages, uses context from request, calls Gemini. """
    if not genai_configured:
        return jsonify({"error": "Chatbot is temporarily unavailable (Configuration Error)"}), 503

    # --- Authentication Check (using simple session) ---
    user_id = session.get('user_id')
    if not user_id:
         logging.warning("Chat API access attempt without session user_id.")
         # Depending on strictness, you might deny or allow anonymous access
         # For this demo, we'll proceed but log it.
         user_id = "anonymous"
    # --------------------------------------------------

    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "Invalid request body"}), 400

        user_message = data.get('message')
        # --- Get context directly from the frontend ---
        original_text_snippet = data.get('originalText')
        analysis_results = data.get('analysisResults') # This should be the analysis JSON object

        if not user_message:
            logging.warning(f"Missing 'message' from user '{user_id}'. Received: {data}")
            return jsonify({"error": "Missing 'message' in request"}), 400

        # Check if context was provided (user might ask general q's before analysis)
        if not original_text_snippet or not analysis_results:
             logging.info(f"Chat request from user '{user_id}' without full context. Answering generally.")
             # Handle general questions or inform user context is missing
             context_prompt_part = "The user has not provided specific document text or analysis results yet. Answer the question generally if possible, or ask them to perform an analysis first."
        else:
            # --- Prepare Context for Prompt ---
            # Safely convert analysis_results dict to string for the prompt
            try:
                analysis_str = json.dumps(analysis_results, indent=2)
            except Exception:
                analysis_str = str(analysis_results) # Fallback to simple string conversion

            context_prompt_part = f"""
Here is the relevant context:

--- Original Text Snippet ---
{original_text_snippet}

--- Analysis Results ---
{analysis_str[:2500]} {'...' if len(analysis_str) > 2500 else ''}
""" # Truncate analysis results string if too long

        # --- Prepare Full Prompt for Gemini ---
        prompt = f"""
You are a helpful AI assistant embedded within the 'SentimentChain' web application.
Your primary role is to answer user questions based *only* on the provided Original Text Snippet and the Analysis Results generated by the SentimentChain tool for that text.
Do not use external knowledge or make up information not present in the provided context.
If the user asks about something not covered in the text or analysis, state that the information is not available in the provided context.
Be concise and helpful.

{context_prompt_part}

--- User Question ---
{user_message}

--- Your Answer ---
"""

        # --- Call Gemini API ---
        try:
            model = genai.GenerativeModel('gemini-2.0-flash')
            response = model.generate_content(
                prompt,
                 safety_settings=[ # Basic safety settings
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                 ]
            )

            # Check for safety blocks or empty responses
            if not response.candidates:
                 feedback = response.prompt_feedback if hasattr(response, 'prompt_feedback') else None
                 block_reason = feedback.block_reason if feedback and hasattr(feedback, 'block_reason') else 'Unknown'
                 logging.warning(f"Gemini response blocked or empty for user '{user_id}'. Reason: {block_reason}. Prompt (start): {prompt[:100]}...")
                 return jsonify({"reply": f"I cannot provide an answer for that request. (Content Policy: {block_reason})"}), 200

            ai_reply = response.text
            logging.info(f"Generated Gemini reply for user '{user_id}'.")

        except Exception as e:
            logging.error(f"Error calling Gemini API for user '{user_id}': {e}")
            return jsonify({"error": "Failed to get response from AI assistant. Please try again later."}), 500

        # --- Return Response ---
        return jsonify({"reply": ai_reply})

    except Exception as e:
        logging.exception(f"An unexpected error occurred in /api/chat: {e}") # Log traceback
        return jsonify({"error": "An internal server error occurred."}), 500

# --- Run the App ---
if __name__ == '__main__':
    # Set debug=False for production
    # Use host='0.0.0.0' to make accessible on your network (use with caution)
    app.run(debug=True, port=5000)